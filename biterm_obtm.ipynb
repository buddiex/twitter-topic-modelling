{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing for user:MrMarketingPhD with 441 tweets\n",
      "Train Online BTM ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [00:49<00:00,  1.00s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [00:31<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "processing for user:Garrettsrock_24 with 441 tweets\n",
      "Train Online BTM ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 20.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "processing for user:AmyFox04307552 with 441 tweets\n",
      "Train Online BTM ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [00:18<00:00,  2.88it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [00:11<00:00,  4.53it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 659.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "processing for user:notyouravgwoman with 441 tweets\n",
      "Train Online BTM ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [00:02<00:00, 19.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys,os\n",
    "import numpy as np\n",
    "import pyLDAvis\n",
    "from biterm.btm import oBTM \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from biterm.utility import vec_to_biterms, topic_summuary # helper functions\n",
    "\n",
    "import pandas as pd\n",
    "import preprocessor as p\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import stop_words\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from textblob import Word\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import wordsegment \n",
    "\n",
    "wordsegment.load()\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "stop_words = stop_words.get_stop_words('en') + nltk.corpus.stopwords.words('english')\n",
    "stop_words = list(set(list(ENGLISH_STOP_WORDS) + stop_words + list(string.ascii_lowercase)))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "wordNet_lemmatizer = lambda x: \" \".join([ lemmatizer.lemmatize(word) for word in x.split()])\n",
    "texblob_lemmatizer = lambda x: \" \".join([Word(word).lemmatize() for word in x.split()])\n",
    "\n",
    "def segment_harsh_tags(tags):\n",
    "    return \" \".join([\" \".join(wordsegment.segment(t)) for t in str(tags).split(\" \") ])\n",
    "\n",
    "def remove_links(tweet):\n",
    "    tweet = re.sub(r'http\\S+', ' ', tweet) # remove http links\n",
    "    tweet = re.sub(r'bit.ly/\\S+', ' ', tweet) # rempve bitly links\n",
    "    tweet = tweet.strip('[link]') # remove [links]\n",
    "    return tweet\n",
    "\n",
    "def remove_users(tweet):\n",
    "    tweet = re.sub(r'(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)', ' ', tweet) # remove retweet\n",
    "    tweet = re.sub(r'(@[A-Za-z]+[A-Za-z0-9-_]+)', ' ', tweet) # remove tweeted at\n",
    "    return tweet\n",
    "\n",
    "def clean_tweets(tweet):\n",
    "    tweet = remove_users(tweet)\n",
    "    tweet = remove_links(tweet)\n",
    "    tweet = tweet.encode('ascii', 'ignore').decode('ascii') # remove emojis\n",
    "    tweet = tweet.lower() # lower case\n",
    "    tweet = re.sub(r'['+string.punctuation+ ']+', ' ', tweet) # strip punctuation\n",
    "    tweet = re.sub(r'\\s+', ' ', tweet) #remove double spacing\n",
    "    tweet = re.sub(r'([0-9]+)', '', tweet) # remove numbers\n",
    "#     tweet = wordNet_lemmatizer(tweet)\n",
    "    tweet_token_list = [word for word in tweet.split(' ')if word not in stop_words and len(word)>2] # remove stopwords\n",
    "    tweet = ' '.join(tweet_token_list)\n",
    "    tweet = p.clean(tweet)\n",
    "    return tweet\n",
    "\n",
    "def get_users_topics(texts,num_topics=10 ):\n",
    "    vec = CountVectorizer(stop_words='english')    # vectorize texts\n",
    "    X = vec.fit_transform(texts).toarray()\n",
    "    vocab = np.array(vec.get_feature_names())     # get vocabulary\n",
    "    biterms = vec_to_biterms(X)    # get biterms\n",
    "    btm = oBTM(num_topics=num_topics, V=vocab)    # create btm\n",
    "\n",
    "    print(\"Train Online BTM ..\")\n",
    "    for i in range(0, len(biterms), 100): # prozess chunk of 200 texts\n",
    "        biterms_chunk = biterms[i:i + 100]\n",
    "        btm.fit(biterms_chunk, iterations=50)\n",
    "    topics = btm.transform(biterms)\n",
    "    summary = topic_summuary(btm.phi_wz.T, X, vocab, num_topics, verbose=False)\n",
    "    return [t.argmax() for t in topics], summary\n",
    "\n",
    "\n",
    "all_user_data_df = pd.read_csv(\"data/tweets_info.csv\")\n",
    "all_user_data_df['segmented_harshtags']=all_user_data_df['harsh_tags'].map(segment_harsh_tags)\n",
    "all_user_data_df['segmented_harshtags']=all_user_data_df['segmented_harshtags'].map(clean_tweets)\n",
    "\n",
    "texts_df = pd.read_csv(\"data/tweets_standalone.csv\")\n",
    "texts_df['clean_tweets'] = texts_df['tweets'].map(clean_tweets) \n",
    "texts_df['topics'] = ''\n",
    "user_topics_df = pd.DataFrame(columns=[\"follower\", \"topic_id\", \"coherence\", \"top_words\", \"theme\"])\n",
    "\n",
    "for user in texts_df['followers_screen_name'].unique():\n",
    "    print(\"processing for user:{} with {} tweets\".format(user, len(texts_df)))\n",
    "\n",
    "    user_query = texts_df['followers_screen_name']==user\n",
    "    text = list(texts_df[user_query]['clean_tweets'])\n",
    "\n",
    "    #actual topic modelling\n",
    "    topics, summary = get_users_topics(text, num_topics=10)\n",
    "\n",
    "    for i in range(len(summary['coherence'])):\n",
    "        user_topics_df=user_topics_df.append({\n",
    "                           \"follower\":user, \n",
    "                            \"topic_id\":i,\n",
    "                           \"coherence\":summary['coherence'][i], \n",
    "                           \"top_words\":\" \".join( summary['top_words'][i])\n",
    "                          }, \n",
    "                          ignore_index=True)\n",
    "    texts_df.loc[user_query, 'topics']=topics\n",
    "#     texts_df[user_query]['topics'] = topics\n",
    "    print(\"\\n\")\n",
    "        \n",
    "#     print(\"\\n\\n Texts & Topics ..\")\n",
    "#     for i in range(len(texts)):\n",
    "#         print(\"{} (topic: {})\".format(texts[i], topics[i].argmax()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_topics_df.to_csv(\"user_topics.csv\")\n",
    "# texts_df.to_csv(\"tweets_user.csv\")\n",
    "\n",
    "topic_freq_df = texts_df\\\n",
    "                    .groupby(['followers_screen_name','topics'],as_index=False)\\\n",
    "                    .size()\\\n",
    "                    .reset_index()\\\n",
    "                    .rename(columns={0: 'topic_freq_per_user'})\n",
    "top_n_topics_per_user = topic_freq_df\\\n",
    "                    .groupby(['followers_screen_name'])['topic_freq_per_user'].nlargest(3)\\\n",
    "                    .reset_index()\\\n",
    "                    .rename(columns={\"level_1\": 'topics'})\n",
    "\n",
    "# top_n_topics_per_user\n",
    "# topic_freq_df\n",
    "# user_topics_df\n",
    "# # texts_df\n",
    "# all_user_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Online BTM ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [00:03<00:00, 13.29it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array(['nsmq', 'thousand', 'xij'], dtype='<U12'),\n",
       " array(['xij', 'city', 'iba'], dtype='<U12'),\n",
       " array(['xij', 'city', 'iba'], dtype='<U12')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = all_user_data_df['segmented_harshtags'][1]\n",
    "topics, summary = get_users_topics([txt], num_topics=3)\n",
    "summary['top_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Train Online BTM ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [01:38<00:00,  1.85s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [00:50<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Visualize Topics ..\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "\n * Not all rows (distributions) in doc_topic_dists sum to 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-63cff92e2dba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\\n Visualize Topics ..\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m     \u001b[0mvis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbtm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mphi_wz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount_nonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m     \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_html\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'./vis/online_btm.html'\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# path to output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py\u001b[0m in \u001b[0;36mprepare\u001b[1;34m(topic_term_dists, doc_topic_dists, doc_lengths, vocab, term_frequency, R, lambda_step, mds, n_jobs, plot_opts, sort_topics)\u001b[0m\n\u001b[0;32m    372\u001b[0m    \u001b[0mdoc_lengths\u001b[0m      \u001b[1;33m=\u001b[0m \u001b[0m_series_with_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_lengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'doc_length'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m    \u001b[0mvocab\u001b[0m            \u001b[1;33m=\u001b[0m \u001b[0m_series_with_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'vocab'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 374\u001b[1;33m    \u001b[0m_input_validate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopic_term_dists\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc_topic_dists\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc_lengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterm_frequency\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    375\u001b[0m    \u001b[0mR\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py\u001b[0m in \u001b[0;36m_input_validate\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m     63\u001b[0m    \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_input_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m    \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValidationError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m' * '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValidationError\u001b[0m: \n * Not all rows (distributions) in doc_topic_dists sum to 1."
     ]
    }
   ],
   "source": [
    "import sys,os\n",
    "import numpy as np\n",
    "import pyLDAvis\n",
    "from biterm.btm import oBTM \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from biterm.utility import vec_to_biterms, topic_summuary # helper functions\n",
    "\n",
    "import pandas as pd\n",
    "import preprocessor as p\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import stop_words\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from textblob import Word\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import wordsegment \n",
    "\n",
    "wordsegment.load()\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "stop_words = stop_words.get_stop_words('en') + nltk.corpus.stopwords.words('english')\n",
    "stop_words = list(set(list(ENGLISH_STOP_WORDS) + stop_words + list(string.ascii_lowercase)))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "wordNet_lemmatizer = lambda x: \" \".join([ lemmatizer.lemmatize(word) for word in x.split()])\n",
    "texblob_lemmatizer = lambda x: \" \".join([Word(word).lemmatize() for word in x.split()])\n",
    "\n",
    "def remove_links(tweet):\n",
    "    tweet = re.sub(r'http\\S+', ' ', tweet) # remove http links\n",
    "    tweet = re.sub(r'bit.ly/\\S+', ' ', tweet) # rempve bitly links\n",
    "    tweet = tweet.strip('[link]') # remove [links]\n",
    "    return tweet\n",
    "\n",
    "def remove_users(tweet):\n",
    "    tweet = re.sub(r'(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)', ' ', tweet) # remove retweet\n",
    "    tweet = re.sub(r'(@[A-Za-z]+[A-Za-z0-9-_]+)', ' ', tweet) # remove tweeted at\n",
    "    return tweet\n",
    "\n",
    "def clean_tweets(tweet):\n",
    "    tweet = remove_users(tweet)\n",
    "    tweet = remove_links(tweet)\n",
    "    tweet = tweet.encode('ascii', 'ignore').decode('ascii') # remove emojis\n",
    "    tweet = tweet.lower() # lower case\n",
    "    tweet = re.sub(r'['+string.punctuation+ ']+', ' ', tweet) # strip punctuation\n",
    "    tweet = re.sub(r'\\s+', ' ', tweet) #remove double spacing\n",
    "    tweet = re.sub(r'([0-9]+)', '', tweet) # remove numbers\n",
    "    tweet = wordNet_lemmatizer(tweet)\n",
    "    tweet = texblob_lemmatizer(tweet)\n",
    "    tweet_token_list = [word for word in tweet.split(' ')if word not in stop_words and len(word)>2] # remove stopwords\n",
    "#     tweet_token_list = [word_rooter(word) if '#' not in word else word for word in tweet_token_list] # apply word rooter\n",
    "#     if bigrams:\n",
    "#         tweet_token_list = tweet_token_list+[tweet_token_list[i]+'_'+tweet_token_list[i+1]\n",
    "#                                             for i in range(len(tweet_token_list)-1)]\n",
    "    tweet = ' '.join(tweet_token_list)\n",
    "    tweet = p.clean(tweet)\n",
    "#     tweet = word_tokenize(tweet)\n",
    "    return tweet\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    texts = open(os.path.realpath('./data/reuters.titles.txt')).read().splitlines() # path of data file\n",
    "    texts = pd.read_csv(\"tweets_standalone.csv\")[\"tweets\"]\n",
    "    texts = [clean_tweets(txt) for txt in list(texts)]\n",
    "    \n",
    "    # vectorize texts\n",
    "    vec = CountVectorizer(stop_words='english')\n",
    "    X = vec.fit_transform(texts).toarray()\n",
    "\n",
    "    # get vocabulary\n",
    "    vocab = np.array(vec.get_feature_names())\n",
    "\n",
    "    # get biterms\n",
    "    biterms = vec_to_biterms(X)\n",
    "\n",
    "    # create btm\n",
    "    btm = oBTM(num_topics=10, V=vocab)\n",
    "\n",
    "    print(\"\\n\\n Train Online BTM ..\")\n",
    "    for i in range(0, len(biterms), 100): # prozess chunk of 200 texts\n",
    "        biterms_chunk = biterms[i:i + 100]\n",
    "        btm.fit(biterms_chunk, iterations=50)\n",
    "    topics = btm.transform(biterms)\n",
    "\n",
    "#     print(\"\\n\\n Visualize Topics ..\")\n",
    "#     vis = pyLDAvis.prepare(btm.phi_wz.T, topics, np.count_nonzero(X, axis=1), vocab, np.sum(X, axis=0))\n",
    "#     pyLDAvis.save_html(vis, './vis/online_btm.html')  # path to output\n",
    "\n",
    "    print(\"\\n\\n Topic coherence ..\")\n",
    "    topic_summuary(btm.phi_wz.T, X, vocab, 10)\n",
    "\n",
    "    print(\"\\n\\n Texts & Topics ..\")\n",
    "    for i in range(len(texts)):\n",
    "        print(\"{} (topic: {})\".format(texts[i], topics[i].argmax()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
